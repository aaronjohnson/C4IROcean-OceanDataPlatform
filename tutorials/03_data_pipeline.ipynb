{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Data Pipeline: File Ingest Workflows\n\nThis notebook demonstrates how to upload files to the Ocean Data Platform and ingest them into queryable tabular datasets.\n\n**What you'll learn:**\n- Create a personal dataset (via web UI)\n- Upload files to an ODP dataset\n- Manage file metadata (name, MIME type, geometry)\n- Ingest files into tabular format\n- Handle different ingest modes (append, truncate, drop)\n\n**Prerequisites:**\n- Running in ODP Workspace (auto-authenticated)\n- An ODP account with access to My Data\n\n## Before You Begin: Create Your Dataset\n\nThe ODP Python SDK requires a dataset UUID to work with. Dataset creation is done via the web interface:\n\n1. Go to [app.hubocean.earth](https://app.hubocean.earth)\n2. Log in with your credentials\n3. Navigate to **My Data** section\n4. Click **Create Dataset** (or similar)\n5. Give it a name (e.g., \"Pipeline Tutorial Data\")\n6. Copy the **UUID** shown in the dataset details\n\nYou'll enter this UUID when running the notebook below.\n\n> **Note:** Programmatic dataset creation (`client.create_dataset()`) and listing personal datasets (`client.list_my_datasets()`) are not currently available in the SDK. See `proposals/my_data_api.md` for a feature proposal."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odp.client import Client\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize ODP client\n",
    "client = Client()\n",
    "print(\"Client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connect to Your Dataset\n",
    "\n",
    "Enter your dataset ID where you have editor permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Enter your dataset UUID from My Data\nDATASET_ID = input(\"Enter your dataset UUID (from app.hubocean.earth → My Data): \").strip()\n\nif not DATASET_ID:\n    raise ValueError(\"Dataset UUID is required. Create one at app.hubocean.earth → My Data\")\n\ndataset = client.dataset(DATASET_ID)\nprint(f\"Connected to dataset: {DATASET_ID}\")\n\n# Check current state\nfiles = dataset.files.list()\nschema = dataset.table.schema()\n\nprint(f\"\\nCurrent state:\")\nprint(f\"  Files: {len(files)}\")\nprint(f\"  Table: {'exists' if schema else 'none'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Sample Oceanographic Data\n",
    "\n",
    "We'll create realistic Norwegian Sea monitoring station data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norwegian Sea monitoring stations\n",
    "np.random.seed(42)  # Reproducible\n",
    "\n",
    "stations = [\n",
    "    {\"id\": \"OSLO-01\", \"name\": \"Oslofjord Outer\", \"lat\": 59.82, \"lon\": 10.52},\n",
    "    {\"id\": \"BERG-01\", \"name\": \"Bergen Harbor\", \"lat\": 60.39, \"lon\": 5.32},\n",
    "    {\"id\": \"STAV-01\", \"name\": \"Stavanger Approach\", \"lat\": 58.97, \"lon\": 5.73},\n",
    "    {\"id\": \"TRND-01\", \"name\": \"Trondheim Fjord\", \"lat\": 63.43, \"lon\": 10.40},\n",
    "    {\"id\": \"TRMS-01\", \"name\": \"Tromsø Sound\", \"lat\": 69.65, \"lon\": 18.96},\n",
    "    {\"id\": \"LOFT-01\", \"name\": \"Lofoten Basin\", \"lat\": 68.50, \"lon\": 14.00},\n",
    "    {\"id\": \"NRTH-01\", \"name\": \"North Cape\", \"lat\": 71.17, \"lon\": 25.78},\n",
    "    {\"id\": \"SVAL-01\", \"name\": \"Svalbard South\", \"lat\": 76.50, \"lon\": 16.00},\n",
    "]\n",
    "\n",
    "# Generate time series (30 days of readings)\n",
    "start_date = datetime(2024, 6, 1)\n",
    "records = []\n",
    "\n",
    "for day in range(30):\n",
    "    timestamp = start_date + timedelta(days=day)\n",
    "    \n",
    "    for station in stations:\n",
    "        # Temperature varies by latitude and season\n",
    "        base_temp = 15 - (station[\"lat\"] - 58) * 0.3\n",
    "        temp = base_temp + np.random.normal(0, 1.5)\n",
    "        \n",
    "        # Salinity relatively stable\n",
    "        salinity = 34.5 + np.random.normal(0, 0.5)\n",
    "        \n",
    "        # Depth varies by station\n",
    "        depth = np.random.choice([5, 10, 25, 50, 100])\n",
    "        \n",
    "        records.append({\n",
    "            \"station_id\": station[\"id\"],\n",
    "            \"station_name\": station[\"name\"],\n",
    "            \"latitude\": station[\"lat\"],\n",
    "            \"longitude\": station[\"lon\"],\n",
    "            \"timestamp\": timestamp.isoformat(),\n",
    "            \"depth_m\": depth,\n",
    "            \"temperature_c\": round(temp, 2),\n",
    "            \"salinity_psu\": round(salinity, 2),\n",
    "            \"dissolved_oxygen_ml_l\": round(6.5 + np.random.normal(0, 0.8), 2),\n",
    "            \"chlorophyll_ug_l\": round(max(0.1, 2.0 + np.random.normal(0, 1.0)), 2)\n",
    "        })\n",
    "\n",
    "df_stations = pd.DataFrame(records)\n",
    "print(f\"Generated {len(df_stations)} observations from {len(stations)} stations over 30 days\")\n",
    "print(f\"\\nColumns: {list(df_stations.columns)}\")\n",
    "df_stations.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Data Summary:\")\n",
    "print(f\"  Date range: {df_stations['timestamp'].min()} to {df_stations['timestamp'].max()}\")\n",
    "print(f\"  Latitude range: {df_stations['latitude'].min():.2f}°N to {df_stations['latitude'].max():.2f}°N\")\n",
    "print(f\"  Temperature range: {df_stations['temperature_c'].min():.1f}°C to {df_stations['temperature_c'].max():.1f}°C\")\n",
    "print(f\"\\nObservations per station:\")\n",
    "print(df_stations['station_name'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload Data as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to CSV bytes\n",
    "csv_buffer = io.BytesIO()\n",
    "df_stations.to_csv(csv_buffer, index=False)\n",
    "csv_bytes = csv_buffer.getvalue()\n",
    "\n",
    "print(f\"CSV size: {len(csv_bytes):,} bytes\")\n",
    "print(f\"\\nFirst 500 chars:\\n{csv_bytes[:500].decode()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to ODP\n",
    "filename = \"norwegian_sea_monitoring_2024.csv\"\n",
    "file_id = dataset.files.upload(filename, csv_bytes)\n",
    "\n",
    "print(f\"Uploaded!\")\n",
    "print(f\"  Filename: {filename}\")\n",
    "print(f\"  File ID: {file_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Update File Metadata\n",
    "\n",
    "Add geographic extent and proper MIME type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bounding box from data\n",
    "min_lon = df_stations['longitude'].min()\n",
    "max_lon = df_stations['longitude'].max()\n",
    "min_lat = df_stations['latitude'].min()\n",
    "max_lat = df_stations['latitude'].max()\n",
    "\n",
    "# Create WKT polygon (with small buffer)\n",
    "bbox_wkt = f\"POLYGON(({min_lon-1} {min_lat-1}, {max_lon+1} {min_lat-1}, {max_lon+1} {max_lat+1}, {min_lon-1} {max_lat+1}, {min_lon-1} {min_lat-1}))\"\n",
    "\n",
    "print(f\"Bounding box: {min_lat:.2f}°N to {max_lat:.2f}°N, {min_lon:.2f}°E to {max_lon:.2f}°E\")\n",
    "print(f\"WKT: {bbox_wkt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update metadata\n",
    "dataset.files.update_meta(file_id, {\n",
    "    \"name\": filename,\n",
    "    \"mime-type\": \"text/csv\",\n",
    "    \"geometry\": bbox_wkt\n",
    "})\n",
    "\n",
    "print(\"Metadata updated!\")\n",
    "\n",
    "# Verify\n",
    "files = dataset.files.list()\n",
    "for f in files:\n",
    "    if f.get('id') == file_id:\n",
    "        print(f\"\\nFile details:\")\n",
    "        print(f\"  Name: {f.get('name')}\")\n",
    "        print(f\"  Size: {f.get('size'):,} bytes\")\n",
    "        print(f\"  MIME: {f.get('mime-type')}\")\n",
    "        print(f\"  Geometry: {f.get('geometry', 'N/A')[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ingest into Table\n",
    "\n",
    "Convert the CSV file into a queryable table.\n",
    "\n",
    "**Ingest modes:**\n",
    "- `drop`: Recreate table from scratch (use for first ingest)\n",
    "- `truncate`: Clear data but keep schema\n",
    "- `append`: Add rows to existing table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest file into table\n",
    "print(\"Ingesting CSV into table...\")\n",
    "\n",
    "dataset.files.ingest(file_id, opt=\"drop\")  # Creates fresh table\n",
    "\n",
    "print(\"Ingest complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify table creation\n",
    "schema = dataset.table.schema()\n",
    "\n",
    "if schema:\n",
    "    print(f\"Table schema ({len(schema)} columns):\")\n",
    "    for field in schema:\n",
    "        print(f\"  {field.name}: {field.type}\")\n",
    "    \n",
    "    stats = dataset.table.stats()\n",
    "    if stats:\n",
    "        print(f\"\\nTable statistics:\")\n",
    "        print(f\"  Rows: {stats.num_rows:,}\")\n",
    "        print(f\"  Size: {stats.size:,} bytes\")\n",
    "else:\n",
    "    print(\"Table not created - check ingest errors above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Query the Ingested Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query all data\n",
    "df_query = dataset.table.select().all().dataframe()\n",
    "\n",
    "print(f\"Queried {len(df_query)} rows\")\n",
    "df_query.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query with filter\n",
    "cold_water = dataset.table.select(\n",
    "    \"temperature_c < $threshold\",\n",
    "    vars={\"threshold\": 5.0}\n",
    ").all().dataframe()\n",
    "\n",
    "print(f\"Cold water observations (<5°C): {len(cold_water)}\")\n",
    "if len(cold_water) > 0:\n",
    "    print(f\"\\nStations with cold water:\")\n",
    "    print(cold_water['station_name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation\n",
    "station_avg = dataset.table.aggregate(\n",
    "    group_by=\"station_name\",\n",
    "    aggr={\n",
    "        \"temperature_c\": \"mean\",\n",
    "        \"salinity_psu\": \"mean\",\n",
    "        \"*\": \"count\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Average conditions by station:\")\n",
    "station_avg.sort_values('mean_temperature_c', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Append Additional Data\n",
    "\n",
    "Add more observations using `append` mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate July data (next 30 days)\n",
    "july_records = []\n",
    "start_july = datetime(2024, 7, 1)\n",
    "\n",
    "for day in range(30):\n",
    "    timestamp = start_july + timedelta(days=day)\n",
    "    \n",
    "    for station in stations:\n",
    "        # Warmer in July\n",
    "        base_temp = 17 - (station[\"lat\"] - 58) * 0.3\n",
    "        temp = base_temp + np.random.normal(0, 1.5)\n",
    "        \n",
    "        july_records.append({\n",
    "            \"station_id\": station[\"id\"],\n",
    "            \"station_name\": station[\"name\"],\n",
    "            \"latitude\": station[\"lat\"],\n",
    "            \"longitude\": station[\"lon\"],\n",
    "            \"timestamp\": timestamp.isoformat(),\n",
    "            \"depth_m\": np.random.choice([5, 10, 25, 50, 100]),\n",
    "            \"temperature_c\": round(temp, 2),\n",
    "            \"salinity_psu\": round(34.5 + np.random.normal(0, 0.5), 2),\n",
    "            \"dissolved_oxygen_ml_l\": round(6.5 + np.random.normal(0, 0.8), 2),\n",
    "            \"chlorophyll_ug_l\": round(max(0.1, 3.0 + np.random.normal(0, 1.2)), 2)  # Higher in summer\n",
    "        })\n",
    "\n",
    "df_july = pd.DataFrame(july_records)\n",
    "print(f\"Generated {len(df_july)} July observations\")\n",
    "print(f\"Temperature range: {df_july['temperature_c'].min():.1f}°C to {df_july['temperature_c'].max():.1f}°C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload July data\n",
    "csv_buffer2 = io.BytesIO()\n",
    "df_july.to_csv(csv_buffer2, index=False)\n",
    "\n",
    "file_id_july = dataset.files.upload(\"norwegian_sea_monitoring_july_2024.csv\", csv_buffer2.getvalue())\n",
    "print(f\"Uploaded July data: {file_id_july}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append to existing table\n",
    "print(\"Appending July data to table...\")\n",
    "\n",
    "dataset.files.ingest(file_id_july, opt=\"append\")\n",
    "\n",
    "# Verify\n",
    "stats = dataset.table.stats()\n",
    "print(f\"\\nTable now has {stats.num_rows:,} rows\")\n",
    "print(f\"Expected: {len(df_stations) + len(df_july)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to verify date range\n",
    "df_all = dataset.table.select(\n",
    "    cols=[\"timestamp\", \"station_name\", \"temperature_c\"]\n",
    ").all().dataframe()\n",
    "\n",
    "print(f\"Full date range: {df_all['timestamp'].min()} to {df_all['timestamp'].max()}\")\n",
    "print(f\"\\nMonthly temperature averages:\")\n",
    "df_all['month'] = pd.to_datetime(df_all['timestamp']).dt.month_name()\n",
    "print(df_all.groupby('month')['temperature_c'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files\n",
    "files = dataset.files.list()\n",
    "print(f\"Files in dataset ({len(files)}):\")\n",
    "for f in files:\n",
    "    print(f\"  {f.get('name', 'unnamed')} - {f.get('size', 0):,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and verify\n",
    "downloaded = b''\n",
    "for chunk in dataset.files.download(file_id):\n",
    "    downloaded += chunk\n",
    "\n",
    "print(f\"Downloaded {len(downloaded):,} bytes\")\n",
    "\n",
    "# Parse back to DataFrame\n",
    "df_downloaded = pd.read_csv(io.BytesIO(downloaded))\n",
    "print(f\"Contains {len(df_downloaded)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup (Optional)\n",
    "\n",
    "Remove test files if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clean up files\n",
    "cleanup = input(\"Delete uploaded files? (yes/no): \").strip().lower()\n",
    "\n",
    "if cleanup == 'yes':\n",
    "    for f in dataset.files.list():\n",
    "        fid = f.get('id')\n",
    "        fname = f.get('name', fid)\n",
    "        dataset.files.delete(fid)\n",
    "        print(f\"Deleted: {fname}\")\n",
    "    \n",
    "    print(f\"\\nRemaining files: {len(dataset.files.list())}\")\n",
    "else:\n",
    "    print(\"Files preserved for further exploration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated a complete data pipeline:\n",
    "\n",
    "1. **Generate**: Create realistic oceanographic monitoring data\n",
    "2. **Upload**: Send CSV to ODP dataset\n",
    "3. **Metadata**: Add geographic extent and MIME type\n",
    "4. **Ingest**: Convert to queryable table (`drop` mode)\n",
    "5. **Query**: Filter and aggregate the data\n",
    "6. **Append**: Add more data incrementally\n",
    "7. **Download**: Retrieve files from ODP\n",
    "\n",
    "## Ingest Mode Reference\n",
    "\n",
    "| Mode | Behavior | Use Case |\n",
    "|------|----------|----------|\n",
    "| `drop` | Delete table, recreate from file | First load, schema changes |\n",
    "| `truncate` | Clear rows, keep schema | Replace all data |\n",
    "| `append` | Add rows to existing | Incremental updates |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **04_multi_dataset_join.ipynb**: Combine multiple datasets\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [ODP Python SDK - Files](https://docs.hubocean.earth/python_sdk/intro/#files)\n",
    "- [ODP My Data](https://app.hubocean.earth/) - Create your own datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}