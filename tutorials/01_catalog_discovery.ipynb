{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catalog Discovery with STAC API\n",
    "\n",
    "This notebook demonstrates how to programmatically discover and explore datasets available on the Ocean Data Platform using the STAC (SpatioTemporal Asset Catalog) API.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Query the STAC API to list available collections\n",
    "- Search for datasets by spatial extent and keywords\n",
    "- Retrieve dataset metadata before loading data\n",
    "- Connect discovered datasets to the Python SDK\n",
    "\n",
    "**Prerequisites:**\n",
    "- Running in ODP Workspace (auto-authenticated) or have an API key\n",
    "- `odp-sdk` installed (`pip install -U odp-sdk`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# STAC API base URL\n",
    "STAC_BASE_URL = \"https://api.hubocean.earth/api/stac\"\n",
    "\n",
    "# Helper function for STAC requests\n",
    "def stac_get(endpoint):\n",
    "    \"\"\"GET request to STAC API endpoint.\"\"\"\n",
    "    url = f\"{STAC_BASE_URL}{endpoint}\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def stac_post(endpoint, payload):\n",
    "    \"\"\"POST request to STAC API endpoint.\"\"\"\n",
    "    url = f\"{STAC_BASE_URL}{endpoint}\"\n",
    "    response = requests.post(url, json=payload)\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the Root Catalog\n",
    "\n",
    "The STAC root catalog provides links to collections and search endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the root catalog\n",
    "root_catalog = stac_get(\"/\")\n",
    "\n",
    "print(\"Catalog ID:\", root_catalog.get(\"id\"))\n",
    "print(\"Description:\", root_catalog.get(\"description\"))\n",
    "print(\"\\nAvailable links:\")\n",
    "for link in root_catalog.get(\"links\", []):\n",
    "    print(f\"  - {link.get('rel')}: {link.get('href')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. List All Collections\n",
    "\n",
    "Collections represent datasets in the STAC model. Each collection has metadata describing its spatial/temporal extent and available assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available collections\n",
    "collections_response = stac_get(\"/collections\")\n",
    "collections = collections_response.get(\"collections\", [])\n",
    "\n",
    "print(f\"Found {len(collections)} collections:\\n\")\n",
    "\n",
    "for coll in collections:\n",
    "    print(f\"ID: {coll.get('id')}\")\n",
    "    print(f\"  Title: {coll.get('title', 'N/A')}\")\n",
    "    print(f\"  Description: {coll.get('description', 'N/A')[:100]}...\")\n",
    "    \n",
    "    # Spatial extent\n",
    "    extent = coll.get(\"extent\", {})\n",
    "    spatial = extent.get(\"spatial\", {}).get(\"bbox\", [])\n",
    "    if spatial:\n",
    "        print(f\"  Bounding Box: {spatial[0]}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Search by Spatial Extent\n",
    "\n",
    "The STAC search endpoint allows filtering by:\n",
    "- **bbox**: Bounding box `[west, south, east, north]`\n",
    "- **intersects**: GeoJSON geometry\n",
    "- **datetime**: ISO 8601 date/time range\n",
    "- **collections**: List of collection IDs to search within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for datasets covering Norwegian waters\n",
    "# Approximate bounding box for Norwegian Sea\n",
    "norwegian_sea_bbox = [-5, 55, 30, 75]  # [west, south, east, north]\n",
    "\n",
    "search_payload = {\n",
    "    \"bbox\": norwegian_sea_bbox,\n",
    "    \"limit\": 10\n",
    "}\n",
    "\n",
    "search_results = stac_post(\"/search\", search_payload)\n",
    "\n",
    "print(f\"Found {len(search_results.get('features', []))} items in Norwegian waters:\\n\")\n",
    "\n",
    "for feature in search_results.get(\"features\", []):\n",
    "    props = feature.get(\"properties\", {})\n",
    "    print(f\"ID: {feature.get('id')}\")\n",
    "    print(f\"  Collection: {feature.get('collection')}\")\n",
    "    print(f\"  Datetime: {props.get('datetime', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Search with GeoJSON Polygon\n",
    "\n",
    "For more precise spatial queries, use a GeoJSON polygon with the `intersects` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define a polygon around the North Sea\nnorth_sea_polygon = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [[\n        [-5, 51],   # SW corner\n        [9, 51],    # SE corner  \n        [9, 62],    # NE corner\n        [-5, 62],   # NW corner\n        [-5, 51]    # Close polygon\n    ]]\n}\n\nsearch_payload = {\n    \"intersects\": north_sea_polygon,\n    \"limit\": 10\n}\n\nsearch_results = stac_post(\"/search\", search_payload)\n\nfeatures = search_results.get(\"features\", [])\nprint(f\"Found {len(features)} items intersecting North Sea polygon:\\n\")\n\n# Collect unique collection IDs from search results\ndiscovered_collections = set()\nfor feature in features:\n    collection_id = feature.get('collection')\n    discovered_collections.add(collection_id)\n    print(f\"  - {feature.get('id')} (collection: {collection_id})\")\n\n# Store first collection for use in subsequent steps\nif discovered_collections:\n    DISCOVERED_COLLECTION_ID = list(discovered_collections)[0]\n    print(f\"\\n>> Using collection '{DISCOVERED_COLLECTION_ID}' for next steps\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Get Detailed Collection Metadata\n\nUsing the collection ID discovered in the previous step, retrieve its full metadata."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get details for the collection discovered in step 5\n\nif 'DISCOVERED_COLLECTION_ID' not in dir():\n    raise ValueError(\n        \"No collection found in step 5. \"\n        \"Re-run the North Sea polygon search above, or try a different region.\"\n    )\n\ncollection_id = DISCOVERED_COLLECTION_ID\n\ntry:\n    collection_detail = stac_get(f\"/collections/{collection_id}\")\n    \n    print(\"Collection Details:\")\n    print(f\"  ID: {collection_detail.get('id')}\")\n    print(f\"  Title: {collection_detail.get('title')}\")\n    print(f\"  Description: {collection_detail.get('description')}\")\n    print(f\"  License: {collection_detail.get('license')}\")\n    \n    # Temporal extent\n    temporal = collection_detail.get(\"extent\", {}).get(\"temporal\", {}).get(\"interval\", [])\n    if temporal:\n        print(f\"  Temporal Range: {temporal[0]}\")\n    \n    # Keywords/tags\n    keywords = collection_detail.get(\"keywords\", [])\n    if keywords:\n        print(f\"  Keywords: {', '.join(keywords)}\")\n        \nexcept requests.exceptions.HTTPError as e:\n    print(f\"Could not fetch collection '{collection_id}': {e}\")\n    print(\"\\nThis may indicate the STAC search returns item IDs that differ from collection IDs.\")\n    print(\"Try using a collection ID from step 3 (List All Collections) instead.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Connect to Python SDK\n",
    "\n",
    "Once you've discovered a dataset via STAC, connect to it using the ODP Python SDK for data access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from odp.client import Client\n\n# Initialize client (auto-authenticated in ODP Workspace)\nclient = Client()\n\n# Connect to the dataset discovered via STAC\n# Use the collection ID from step 6\ndataset = client.dataset(collection_id)\n\n# Get schema to understand the data structure\nschema = dataset.table.schema()\nif schema:\n    print(f\"Dataset Schema for {collection_id}:\")\n    for field in schema:\n        print(f\"  {field.name}: {field.type}\")\nelse:\n    print(\"This dataset may be file-based rather than tabular.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get table statistics\n",
    "stats = dataset.table.stats()\n",
    "if stats:\n",
    "    print(f\"Total rows: {stats.num_rows:,}\")\n",
    "    print(f\"Size: {stats.size:,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first few rows\n",
    "preview_df = dataset.table.select().all(max_rows=5).dataframe()\n",
    "preview_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Build a Dataset Inventory\n",
    "\n",
    "Create a summary inventory of available datasets for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build inventory from collections\n",
    "inventory = []\n",
    "\n",
    "for coll in collections:\n",
    "    extent = coll.get(\"extent\", {})\n",
    "    spatial = extent.get(\"spatial\", {}).get(\"bbox\", [[]])[0] if extent.get(\"spatial\", {}).get(\"bbox\") else None\n",
    "    temporal = extent.get(\"temporal\", {}).get(\"interval\", [[]])[0] if extent.get(\"temporal\", {}).get(\"interval\") else None\n",
    "    \n",
    "    inventory.append({\n",
    "        \"id\": coll.get(\"id\"),\n",
    "        \"title\": coll.get(\"title\", \"N/A\"),\n",
    "        \"description\": coll.get(\"description\", \"N/A\")[:100] + \"...\" if coll.get(\"description\") else \"N/A\",\n",
    "        \"license\": coll.get(\"license\", \"N/A\"),\n",
    "        \"bbox\": str(spatial) if spatial else \"N/A\",\n",
    "        \"temporal_start\": temporal[0] if temporal else \"N/A\",\n",
    "        \"temporal_end\": temporal[1] if temporal and len(temporal) > 1 else \"N/A\",\n",
    "        \"keywords\": \", \".join(coll.get(\"keywords\", []))\n",
    "    })\n",
    "\n",
    "inventory_df = pd.DataFrame(inventory)\n",
    "print(f\"Dataset Inventory ({len(inventory_df)} collections):\")\n",
    "inventory_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save inventory to CSV for reference\n",
    "inventory_df.to_csv(\"odp_dataset_inventory.csv\", index=False)\n",
    "print(\"Inventory saved to odp_dataset_inventory.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've discovered available datasets, continue with:\n",
    "\n",
    "- **02_geospatial_analysis.ipynb**: Query and visualize data using H3 hexagonal aggregation\n",
    "- **03_data_pipeline.ipynb**: Ingest files and transform into tabular data\n",
    "- **04_multi_dataset_join.ipynb**: Combine multiple datasets for analysis\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [ODP Documentation](https://docs.hubocean.earth/)\n",
    "- [STAC Specification](https://stacspec.org/)\n",
    "- [Python SDK Reference](https://docs.hubocean.earth/python_sdk/intro/)\n",
    "- [ODP Catalog (Web UI)](https://app.hubocean.earth/catalog)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}