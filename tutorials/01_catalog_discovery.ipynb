{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catalog Discovery with STAC API\n",
    "\n",
    "This notebook demonstrates how to programmatically discover and explore datasets available on the Ocean Data Platform using the STAC (SpatioTemporal Asset Catalog) API.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Query the STAC API to list available collections\n",
    "- Search for datasets by spatial extent and keywords\n",
    "- Retrieve dataset metadata before loading data\n",
    "- Connect discovered datasets to the Python SDK\n",
    "\n",
    "**Prerequisites:**\n",
    "- Running in ODP Workspace (auto-authenticated) or have an API key\n",
    "- `odp-sdk` installed (`pip install -U odp-sdk`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# STAC API base URL\n",
    "STAC_BASE_URL = \"https://api.hubocean.earth/api/stac\"\n",
    "\n",
    "# Helper function for STAC requests\n",
    "def stac_get(endpoint):\n",
    "    \"\"\"GET request to STAC API endpoint.\"\"\"\n",
    "    url = f\"{STAC_BASE_URL}{endpoint}\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def stac_post(endpoint, payload):\n",
    "    \"\"\"POST request to STAC API endpoint.\"\"\"\n",
    "    url = f\"{STAC_BASE_URL}{endpoint}\"\n",
    "    response = requests.post(url, json=payload)\n",
    "    response.raise_for_status()\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the Root Catalog\n",
    "\n",
    "The STAC root catalog provides links to collections and search endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the root catalog\n",
    "root_catalog = stac_get(\"/\")\n",
    "\n",
    "print(\"Catalog ID:\", root_catalog.get(\"id\"))\n",
    "print(\"Description:\", root_catalog.get(\"description\"))\n",
    "print(\"\\nAvailable links:\")\n",
    "for link in root_catalog.get(\"links\", []):\n",
    "    print(f\"  - {link.get('rel')}: {link.get('href')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. List All Collections\n",
    "\n",
    "Collections represent datasets in the STAC model. Each collection has metadata describing its spatial/temporal extent and available assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available collections\n",
    "collections_response = stac_get(\"/collections\")\n",
    "collections = collections_response.get(\"collections\", [])\n",
    "\n",
    "print(f\"Found {len(collections)} collections:\\n\")\n",
    "\n",
    "for coll in collections:\n",
    "    print(f\"ID: {coll.get('id')}\")\n",
    "    print(f\"  Title: {coll.get('title', 'N/A')}\")\n",
    "    print(f\"  Description: {coll.get('description', 'N/A')[:100]}...\")\n",
    "    \n",
    "    # Spatial extent\n",
    "    extent = coll.get(\"extent\", {})\n",
    "    spatial = extent.get(\"spatial\", {}).get(\"bbox\", [])\n",
    "    if spatial:\n",
    "        print(f\"  Bounding Box: {spatial[0]}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Search by Spatial Extent\n",
    "\n",
    "The STAC search endpoint allows filtering by:\n",
    "- **bbox**: Bounding box `[west, south, east, north]`\n",
    "- **intersects**: GeoJSON geometry\n",
    "- **datetime**: ISO 8601 date/time range\n",
    "- **collections**: List of collection IDs to search within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for datasets covering Norwegian waters\n",
    "# Approximate bounding box for Norwegian Sea\n",
    "norwegian_sea_bbox = [-5, 55, 30, 75]  # [west, south, east, north]\n",
    "\n",
    "search_payload = {\n",
    "    \"bbox\": norwegian_sea_bbox,\n",
    "    \"limit\": 10\n",
    "}\n",
    "\n",
    "search_results = stac_post(\"/search\", search_payload)\n",
    "\n",
    "print(f\"Found {len(search_results.get('features', []))} items in Norwegian waters:\\n\")\n",
    "\n",
    "for feature in search_results.get(\"features\", []):\n",
    "    props = feature.get(\"properties\", {})\n",
    "    print(f\"ID: {feature.get('id')}\")\n",
    "    print(f\"  Collection: {feature.get('collection')}\")\n",
    "    print(f\"  Datetime: {props.get('datetime', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Search with GeoJSON Polygon\n",
    "\n",
    "For more precise spatial queries, use a GeoJSON polygon with the `intersects` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define a polygon around the North Sea / Norwegian waters\nnorth_sea_polygon = {\n    \"type\": \"Polygon\",\n    \"coordinates\": [[\n        [-5, 51],   # SW corner\n        [15, 51],   # SE corner (expanded east)\n        [15, 72],   # NE corner (expanded north)\n        [-5, 72],   # NW corner\n        [-5, 51]    # Close polygon\n    ]]\n}\n\nsearch_payload = {\n    \"intersects\": north_sea_polygon,\n    \"limit\": 100  # Increased limit\n}\n\nsearch_results = stac_post(\"/search\", search_payload)\n\nfeatures = search_results.get(\"features\", [])\nprint(f\"Found {len(features)} items intersecting search polygon\\n\")\n\n# Collect unique collection IDs from spatial search\ndiscovered_collection_ids = list(set(\n    feature.get('collection') for feature in features if feature.get('collection')\n))\n\n# Include known working datasets that may not appear in spatial search\n# (global datasets or those with different spatial indexing)\nknown_datasets = [\n    \"b960c80e-7ead-47af-b6c8-e92a9b5ac659\",  # PGS Brazil - Biota and Physics (TABULAR)\n    \"15dac249-4e3d-474b-a246-ba95cffc8807\",  # GLODAP - ocean chemistry\n    \"5070af58-6d8a-4636-a6a0-8ca9298fb3ab\",  # GEBCO Bathymetry\n]\n\nfor kd in known_datasets:\n    if kd not in discovered_collection_ids:\n        discovered_collection_ids.append(kd)\n\nprint(f\"Total: {len(discovered_collection_ids)} unique collections to probe\")\nprint(\"  (includes known working datasets)\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "source": "from odp.client import Client\n\n# Initialize ODP client to probe dataset types\nclient = Client()\n\n# Probe each collection to determine type (tabular vs file-based)\ncollection_info = []\n\nprint(\"Probing collections to detect types...\")\nfor coll_id in discovered_collection_ids:\n    try:\n        # Get STAC metadata for title\n        coll_meta = stac_get(f\"/collections/{coll_id}\")\n        title = coll_meta.get('title', 'Unknown')[:50]\n        \n        # Probe ODP to detect type\n        ds = client.dataset(coll_id)\n        schema = ds.table.schema()\n        \n        if schema:\n            dtype = \"TABULAR\"\n            stats = ds.table.stats()\n            if stats and stats.num_rows:\n                detail = f\"{stats.num_rows:,} rows\"\n            else:\n                detail = f\"{len(schema)} columns\"\n        else:\n            dtype = \"FILE\"\n            files = ds.files.list()\n            if files:\n                detail = f\"{len(files)} files\"\n            else:\n                # No files accessible - might be permissions or different access pattern\n                detail = \"no direct access\"\n        \n        collection_info.append({\n            'id': coll_id,\n            'title': title,\n            'type': dtype,\n            'detail': detail\n        })\n        print(f\"  ✓ {title[:30]}... ({dtype}: {detail})\")\n    except Exception as e:\n        print(f\"  ✗ {coll_id[:20]}... (skipped: {str(e)[:30]})\")\n\n# Display numbered list for selection\nprint(f\"\\n{'='*70}\")\nprint(f\"Discovered {len(collection_info)} accessible collections:\\n\")\nprint(f\"{'#':<3} {'Type':<8} {'Details':<18} {'Title'}\")\nprint(\"-\" * 70)\n\nfor i, info in enumerate(collection_info):\n    print(f\"{i+1:<3} {info['type']:<8} {info['detail']:<18} {info['title']}\")\n\n# Note about FILE datasets\nfile_datasets = [c for c in collection_info if c['type'] == 'FILE']\nif file_datasets:\n    print(f\"\\nNote: FILE datasets with 'no direct access' may require different\")\n    print(f\"      access methods or permissions. Try TABULAR datasets first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Select a dataset to explore\nprint(\"Enter a number from the list above (or press Enter for #1):\")\nchoice = input(\"Selection: \").strip()\n\n# Default to first item if empty\nif not choice:\n    choice = \"1\"\n\ntry:\n    idx = int(choice) - 1\n    if 0 <= idx < len(collection_info):\n        selected = collection_info[idx]\n        collection_id = selected['id']\n        is_tabular = selected['type'] == 'TABULAR'\n        \n        print(f\"\\nSelected: {selected['title']}\")\n        print(f\"Type: {selected['type']} ({selected['detail']})\")\n        print(f\"ID: {collection_id}\")\n    else:\n        print(f\"Invalid selection. Please choose 1-{len(collection_info)}\")\nexcept ValueError:\n    print(\"Please enter a valid number\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 6. Select and Explore Dataset\n\nSelect a dataset from the discovered list to examine its structure.",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Connect to selected dataset (client already initialized)\nif 'collection_id' not in dir() or collection_id is None:\n    print(\"No dataset selected. Run the selection cell above first.\")\nelse:\n    dataset = client.dataset(collection_id)\n\n    print(f\"Dataset: {selected['title']}\")\n    print(f\"Type: {selected['type']}\")\n    print(f\"ID: {collection_id}\")\n    print()\n\n    if is_tabular:\n        schema = dataset.table.schema()\n        print(f\"Schema ({len(schema)} columns):\")\n        for field in schema:\n            print(f\"  {field.name}: {field.type}\")\n    else:\n        files = dataset.files.list()\n        print(f\"Contains {len(files)} files\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get dataset statistics (branched by type)\nif 'is_tabular' not in dir():\n    print(\"No dataset selected. Run the selection cell above first.\")\nelif is_tabular:\n    stats = dataset.table.stats()\n    if stats:\n        print(\"Table Statistics:\")\n        print(f\"  Total rows: {stats.num_rows:,}\")\n        print(f\"  Size: {stats.size:,} bytes\")\nelse:\n    # File-based dataset: list files\n    files = dataset.files.list()\n    print(f\"Files in dataset ({len(files)} total):\\n\")\n    for f in files[:10]:  # Show first 10\n        print(f\"  ID: {f.get('id')}\")\n        print(f\"    Name: {f.get('name', 'N/A')}\")\n        print(f\"    Size: {f.get('size', 'N/A')} bytes\")\n        print(f\"    MIME: {f.get('mime-type', 'N/A')}\")\n        print()\n    if len(files) > 10:\n        print(f\"  ... and {len(files) - 10} more files\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Preview data (branched by type)\nfrom IPython.display import display\n\nif 'is_tabular' not in dir():\n    print(\"No dataset selected. Run the selection cell above first.\")\nelif is_tabular:\n    # Preview first few rows of tabular data\n    preview_df = dataset.table.select().all(max_rows=5).dataframe()\n    print(\"Preview (first 5 rows):\")\n    display(preview_df)\nelse:\n    # For file-based: show how to download a file\n    files = dataset.files.list()\n    if files:\n        first_file = files[0]\n        file_id = first_file.get('id')\n        file_name = first_file.get('name', 'downloaded_file')\n        \n        print(f\"Example: Download first file '{file_name}'\")\n        print(f\"File ID: {file_id}\")\n        print(f\"\\nTo download, run:\")\n        print(f\"  with open('{file_name}', 'wb') as f:\")\n        print(f\"      for chunk in dataset.files.download('{file_id}'):\")\n        print(f\"          f.write(chunk)\")\n    else:\n        print(\"No files found in this dataset.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Build a Dataset Inventory\n",
    "\n",
    "Create a summary inventory of available datasets for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build inventory from collections\n",
    "inventory = []\n",
    "\n",
    "for coll in collections:\n",
    "    extent = coll.get(\"extent\", {})\n",
    "    spatial = extent.get(\"spatial\", {}).get(\"bbox\", [[]])[0] if extent.get(\"spatial\", {}).get(\"bbox\") else None\n",
    "    temporal = extent.get(\"temporal\", {}).get(\"interval\", [[]])[0] if extent.get(\"temporal\", {}).get(\"interval\") else None\n",
    "    \n",
    "    inventory.append({\n",
    "        \"id\": coll.get(\"id\"),\n",
    "        \"title\": coll.get(\"title\", \"N/A\"),\n",
    "        \"description\": coll.get(\"description\", \"N/A\")[:100] + \"...\" if coll.get(\"description\") else \"N/A\",\n",
    "        \"license\": coll.get(\"license\", \"N/A\"),\n",
    "        \"bbox\": str(spatial) if spatial else \"N/A\",\n",
    "        \"temporal_start\": temporal[0] if temporal else \"N/A\",\n",
    "        \"temporal_end\": temporal[1] if temporal and len(temporal) > 1 else \"N/A\",\n",
    "        \"keywords\": \", \".join(coll.get(\"keywords\", []))\n",
    "    })\n",
    "\n",
    "inventory_df = pd.DataFrame(inventory)\n",
    "print(f\"Dataset Inventory ({len(inventory_df)} collections):\")\n",
    "inventory_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save inventory to CSV for reference\n",
    "inventory_df.to_csv(\"odp_dataset_inventory.csv\", index=False)\n",
    "print(\"Inventory saved to odp_dataset_inventory.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you've discovered available datasets, continue with:\n",
    "\n",
    "- **02_geospatial_analysis.ipynb**: Query and visualize data using H3 hexagonal aggregation\n",
    "- **03_data_pipeline.ipynb**: Ingest files and transform into tabular data\n",
    "- **04_multi_dataset_join.ipynb**: Combine multiple datasets for analysis\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [ODP Documentation](https://docs.hubocean.earth/)\n",
    "- [STAC Specification](https://stacspec.org/)\n",
    "- [Python SDK Reference](https://docs.hubocean.earth/python_sdk/intro/)\n",
    "- [ODP Catalog (Web UI)](https://app.hubocean.earth/catalog)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}