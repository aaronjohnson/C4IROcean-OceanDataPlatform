{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Dataset Analysis: Combining Ocean Data Sources\n",
    "\n",
    "This notebook demonstrates how to combine data from multiple ODP datasets for integrated analysis.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Load and explore multiple datasets\n",
    "- Join data by common keys (temporal, spatial, categorical)\n",
    "- Perform cross-dataset analysis\n",
    "- Handle different data resolutions and formats\n",
    "\n",
    "**Prerequisites:**\n",
    "- Running in ODP Workspace (auto-authenticated)\n",
    "- Completed previous tutorials (01-03)\n",
    "\n",
    "## Why Combine Datasets?\n",
    "\n",
    "Ocean science often requires integrating multiple data sources:\n",
    "- **Bathymetry + Biology**: Correlate species distribution with seafloor depth\n",
    "- **Temperature + Chemistry**: Link physical and chemical oceanography\n",
    "- **Observations + Models**: Validate model predictions against measurements\n",
    "- **Multi-temporal**: Compare conditions across time periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odp.client import Client\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# Initialize ODP client\n",
    "client = Client()\n",
    "\n",
    "# STAC API for dataset discovery\n",
    "STAC_API = \"https://api.hubocean.earth/api/stac\"\n",
    "\n",
    "print(\"Client initialized\")\n",
    "\n",
    "# Optional visualization\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    HAS_MPL = True\n",
    "except ImportError:\n",
    "    HAS_MPL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Discover Related Datasets\n",
    "\n",
    "Use STAC API to find datasets in a common geographic region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for datasets in Norwegian waters\n",
    "norwegian_bbox = [0, 55, 35, 75]  # [west, south, east, north]\n",
    "\n",
    "search_params = {\n",
    "    \"bbox\": norwegian_bbox,\n",
    "    \"limit\": 50\n",
    "}\n",
    "\n",
    "response = requests.post(f\"{STAC_API}/search\", json=search_params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    results = response.json()\n",
    "    features = results.get('features', [])\n",
    "    print(f\"Found {len(features)} datasets in Norwegian waters\\n\")\n",
    "    \n",
    "    # Display available datasets\n",
    "    for i, f in enumerate(features[:15]):\n",
    "        props = f.get('properties', {})\n",
    "        title = props.get('title', f['id'][:40])\n",
    "        desc = props.get('description', 'No description')[:60]\n",
    "        print(f\"{i+1:2}. {title}\")\n",
    "        print(f\"    {desc}...\")\n",
    "else:\n",
    "    print(f\"Search failed: {response.status_code}\")\n",
    "    features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to probe dataset type\n",
    "def get_dataset_info(dataset_id):\n",
    "    \"\"\"Get dataset type and basic info.\"\"\"\n",
    "    try:\n",
    "        ds = client.dataset(dataset_id)\n",
    "        schema = ds.table.schema()\n",
    "        files = ds.files.list()\n",
    "        \n",
    "        info = {\n",
    "            'id': dataset_id,\n",
    "            'is_tabular': schema is not None,\n",
    "            'columns': [f.name for f in schema] if schema else [],\n",
    "            'file_count': len(files) if files else 0\n",
    "        }\n",
    "        \n",
    "        if schema:\n",
    "            stats = ds.table.stats()\n",
    "            info['row_count'] = stats.num_rows if stats else 0\n",
    "        \n",
    "        return info\n",
    "    except Exception as e:\n",
    "        return {'id': dataset_id, 'error': str(e)}\n",
    "\n",
    "# Probe a few datasets\n",
    "print(\"Probing dataset types...\\n\")\n",
    "for f in features[:5]:\n",
    "    info = get_dataset_info(f['id'])\n",
    "    dtype = 'TABULAR' if info.get('is_tabular') else 'FILE'\n",
    "    title = f.get('properties', {}).get('title', f['id'][:30])\n",
    "    print(f\"  {title}: {dtype}\")\n",
    "    if info.get('columns'):\n",
    "        print(f\"    Columns: {info['columns'][:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Datasets for Analysis\n",
    "\n",
    "We'll work with two complementary dataset types. If tabular datasets aren't available, we'll demonstrate the pattern with synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select datasets to combine\n",
    "# Option 1: Use discovered tabular datasets\n",
    "# Option 2: Use user's personal datasets\n",
    "# Option 3: Generate synthetic demo data\n",
    "\n",
    "print(\"Dataset Selection Options:\")\n",
    "print(\"1. Enter two dataset UUIDs to combine\")\n",
    "print(\"2. Use synthetic demo data (recommended for learning)\")\n",
    "\n",
    "choice = input(\"\\nChoice (1 or 2): \").strip()\n",
    "\n",
    "USE_SYNTHETIC = choice != '1'\n",
    "\n",
    "if not USE_SYNTHETIC:\n",
    "    DATASET_1 = input(\"Dataset 1 UUID: \").strip()\n",
    "    DATASET_2 = input(\"Dataset 2 UUID: \").strip()\n",
    "    print(f\"\\nUsing datasets: {DATASET_1[:8]}... and {DATASET_2[:8]}...\")\n",
    "else:\n",
    "    print(\"\\nUsing synthetic demo data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic oceanographic data for demonstration\n",
    "if USE_SYNTHETIC:\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Dataset 1: Temperature observations from monitoring stations\n",
    "    stations = [\n",
    "        {\"id\": \"ST001\", \"lat\": 60.5, \"lon\": 5.0, \"name\": \"Bergen Offshore\"},\n",
    "        {\"id\": \"ST002\", \"lat\": 63.0, \"lon\": 8.5, \"name\": \"Trondheim Deep\"},\n",
    "        {\"id\": \"ST003\", \"lat\": 66.5, \"lon\": 13.0, \"name\": \"Arctic Gateway\"},\n",
    "        {\"id\": \"ST004\", \"lat\": 70.0, \"lon\": 20.0, \"name\": \"Barents Entry\"},\n",
    "    ]\n",
    "    \n",
    "    temp_records = []\n",
    "    for month in range(1, 13):\n",
    "        for station in stations:\n",
    "            # Temperature varies by latitude and season\n",
    "            base_temp = 12 - (station['lat'] - 58) * 0.2\n",
    "            seasonal = 4 * np.sin((month - 3) * np.pi / 6)  # Peak in August\n",
    "            temp = base_temp + seasonal + np.random.normal(0, 1)\n",
    "            \n",
    "            temp_records.append({\n",
    "                'station_id': station['id'],\n",
    "                'station_name': station['name'],\n",
    "                'latitude': station['lat'],\n",
    "                'longitude': station['lon'],\n",
    "                'month': month,\n",
    "                'temperature_c': round(temp, 2),\n",
    "                'salinity_psu': round(34.5 + np.random.normal(0, 0.3), 2)\n",
    "            })\n",
    "    \n",
    "    df_temperature = pd.DataFrame(temp_records)\n",
    "    print(f\"Temperature dataset: {len(df_temperature)} records\")\n",
    "    print(f\"Stations: {df_temperature['station_name'].unique().tolist()}\")\n",
    "    df_temperature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SYNTHETIC:\n",
    "    # Dataset 2: Species observations (biota)\n",
    "    species_list = [\n",
    "        {\"name\": \"Atlantic Cod\", \"temp_min\": 2, \"temp_max\": 12},\n",
    "        {\"name\": \"Atlantic Herring\", \"temp_min\": 4, \"temp_max\": 14},\n",
    "        {\"name\": \"Mackerel\", \"temp_min\": 8, \"temp_max\": 18},\n",
    "        {\"name\": \"Capelin\", \"temp_min\": 0, \"temp_max\": 8},\n",
    "    ]\n",
    "    \n",
    "    bio_records = []\n",
    "    for month in range(1, 13):\n",
    "        for station in stations:\n",
    "            # Get approximate temperature at this station/month\n",
    "            base_temp = 12 - (station['lat'] - 58) * 0.2\n",
    "            seasonal = 4 * np.sin((month - 3) * np.pi / 6)\n",
    "            local_temp = base_temp + seasonal\n",
    "            \n",
    "            # Species presence depends on temperature preference\n",
    "            for species in species_list:\n",
    "                if species['temp_min'] <= local_temp <= species['temp_max']:\n",
    "                    # Species is present - generate observation\n",
    "                    abundance = np.random.poisson(50)  # Count\n",
    "                    if abundance > 0:\n",
    "                        bio_records.append({\n",
    "                            'station_id': station['id'],\n",
    "                            'month': month,\n",
    "                            'species': species['name'],\n",
    "                            'abundance': abundance,\n",
    "                            'biomass_kg': round(abundance * np.random.uniform(0.5, 5.0), 1)\n",
    "                        })\n",
    "    \n",
    "    df_biology = pd.DataFrame(bio_records)\n",
    "    print(f\"Biology dataset: {len(df_biology)} records\")\n",
    "    print(f\"Species: {df_biology['species'].unique().tolist()}\")\n",
    "    df_biology.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from real ODP datasets if available\n",
    "if not USE_SYNTHETIC:\n",
    "    try:\n",
    "        ds1 = client.dataset(DATASET_1)\n",
    "        ds2 = client.dataset(DATASET_2)\n",
    "        \n",
    "        # Try to load tabular data\n",
    "        schema1 = ds1.table.schema()\n",
    "        schema2 = ds2.table.schema()\n",
    "        \n",
    "        if schema1:\n",
    "            df_dataset1 = ds1.table.select().all(max_rows=10000).dataframe()\n",
    "            print(f\"Dataset 1: {len(df_dataset1)} rows, columns: {list(df_dataset1.columns)}\")\n",
    "        else:\n",
    "            print(f\"Dataset 1 is file-based, not tabular\")\n",
    "            df_dataset1 = None\n",
    "            \n",
    "        if schema2:\n",
    "            df_dataset2 = ds2.table.select().all(max_rows=10000).dataframe()\n",
    "            print(f\"Dataset 2: {len(df_dataset2)} rows, columns: {list(df_dataset2.columns)}\")\n",
    "        else:\n",
    "            print(f\"Dataset 2 is file-based, not tabular\")\n",
    "            df_dataset2 = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading datasets: {e}\")\n",
    "        print(\"Falling back to synthetic data\")\n",
    "        USE_SYNTHETIC = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Join Strategies\n",
    "\n",
    "Common approaches for combining oceanographic datasets:\n",
    "\n",
    "| Join Type | Key Column(s) | Use Case |\n",
    "|-----------|---------------|----------|\n",
    "| **Exact** | station_id, timestamp | Same sampling locations |\n",
    "| **Temporal** | date/month/year | Time-aligned data |\n",
    "| **Spatial** | lat/lon bins, H3 hex | Nearby observations |\n",
    "| **Categorical** | region, species | Group-based analysis |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Exact key join (station + time)\n",
    "if USE_SYNTHETIC:\n",
    "    # Join temperature and biology on station_id + month\n",
    "    df_combined = pd.merge(\n",
    "        df_biology,\n",
    "        df_temperature,\n",
    "        on=['station_id', 'month'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"Combined dataset: {len(df_combined)} records\")\n",
    "    print(f\"Columns: {list(df_combined.columns)}\")\n",
    "    df_combined.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Spatial binning join\n",
    "# When datasets don't share exact coordinates, bin to a common grid\n",
    "\n",
    "def bin_coordinates(lat, lon, resolution=1.0):\n",
    "    \"\"\"Bin lat/lon to grid cells.\"\"\"\n",
    "    lat_bin = np.floor(lat / resolution) * resolution\n",
    "    lon_bin = np.floor(lon / resolution) * resolution\n",
    "    return f\"{lat_bin:.1f}_{lon_bin:.1f}\"\n",
    "\n",
    "if USE_SYNTHETIC:\n",
    "    # Add spatial bins to temperature data\n",
    "    df_temperature['spatial_bin'] = df_temperature.apply(\n",
    "        lambda row: bin_coordinates(row['latitude'], row['longitude'], resolution=2.0),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    print(\"Spatial bins in temperature data:\")\n",
    "    print(df_temperature['spatial_bin'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Dataset Analysis\n",
    "\n",
    "Now we can analyze relationships between temperature and species distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SYNTHETIC:\n",
    "    # Analyze: How does temperature affect species abundance?\n",
    "    species_temp_analysis = df_combined.groupby('species').agg({\n",
    "        'temperature_c': ['mean', 'min', 'max'],\n",
    "        'abundance': ['sum', 'mean'],\n",
    "        'station_id': 'count'\n",
    "    }).round(2)\n",
    "    \n",
    "    species_temp_analysis.columns = ['_'.join(col) for col in species_temp_analysis.columns]\n",
    "    species_temp_analysis = species_temp_analysis.rename(columns={'station_id_count': 'observations'})\n",
    "    \n",
    "    print(\"Species Temperature Preferences (from combined data):\")\n",
    "    print(species_temp_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SYNTHETIC:\n",
    "    # Seasonal patterns by species\n",
    "    seasonal_abundance = df_combined.groupby(['species', 'month']).agg({\n",
    "        'abundance': 'sum',\n",
    "        'temperature_c': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    print(\"Seasonal abundance patterns:\")\n",
    "    for species in df_combined['species'].unique():\n",
    "        sp_data = seasonal_abundance[seasonal_abundance['species'] == species]\n",
    "        peak_month = sp_data.loc[sp_data['abundance'].idxmax(), 'month']\n",
    "        peak_temp = sp_data.loc[sp_data['abundance'].idxmax(), 'temperature_c']\n",
    "        print(f\"  {species}: Peak in month {peak_month} (temp: {peak_temp:.1f}°C)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SYNTHETIC and HAS_MPL:\n",
    "    # Visualize temperature-abundance relationship\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    for idx, species in enumerate(df_combined['species'].unique()):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        sp_data = df_combined[df_combined['species'] == species]\n",
    "        \n",
    "        ax.scatter(sp_data['temperature_c'], sp_data['abundance'], alpha=0.6)\n",
    "        ax.set_xlabel('Temperature (°C)')\n",
    "        ax.set_ylabel('Abundance')\n",
    "        ax.set_title(species)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Species Abundance vs Temperature', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "elif USE_SYNTHETIC:\n",
    "    print(\"Install matplotlib for visualization: pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Spatial Correlation Analysis\n",
    "\n",
    "Analyze how patterns vary across stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SYNTHETIC:\n",
    "    # Station-level summary\n",
    "    station_summary = df_combined.groupby(['station_id', 'station_name', 'latitude']).agg({\n",
    "        'temperature_c': 'mean',\n",
    "        'abundance': 'sum',\n",
    "        'species': 'nunique'\n",
    "    }).reset_index()\n",
    "    \n",
    "    station_summary.columns = ['station_id', 'station_name', 'latitude', \n",
    "                                'avg_temp', 'total_abundance', 'species_richness']\n",
    "    \n",
    "    station_summary = station_summary.sort_values('latitude')\n",
    "    \n",
    "    print(\"Station Summary (sorted by latitude):\")\n",
    "    print(station_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SYNTHETIC:\n",
    "    # Correlation between temperature and biodiversity\n",
    "    monthly_biodiversity = df_combined.groupby(['station_id', 'month']).agg({\n",
    "        'temperature_c': 'first',\n",
    "        'species': 'nunique',\n",
    "        'abundance': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate correlation\n",
    "    temp_richness_corr = monthly_biodiversity['temperature_c'].corr(\n",
    "        monthly_biodiversity['species']\n",
    "    )\n",
    "    temp_abundance_corr = monthly_biodiversity['temperature_c'].corr(\n",
    "        monthly_biodiversity['abundance']\n",
    "    )\n",
    "    \n",
    "    print(f\"Temperature-Biodiversity Correlations:\")\n",
    "    print(f\"  Temperature vs Species Richness: {temp_richness_corr:.3f}\")\n",
    "    print(f\"  Temperature vs Total Abundance:  {temp_abundance_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Combined Data\n",
    "\n",
    "Save the combined dataset for further analysis or upload to ODP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SYNTHETIC:\n",
    "    # Export to CSV\n",
    "    output_file = \"combined_temp_biology_analysis.csv\"\n",
    "    df_combined.to_csv(output_file, index=False)\n",
    "    print(f\"Exported {len(df_combined)} records to {output_file}\")\n",
    "    \n",
    "    # Show file size\n",
    "    import os\n",
    "    size_kb = os.path.getsize(output_file) / 1024\n",
    "    print(f\"File size: {size_kb:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Upload combined data to your ODP dataset\n",
    "upload_choice = input(\"Upload combined data to ODP? Enter dataset UUID or 'skip': \").strip()\n",
    "\n",
    "if upload_choice and upload_choice.lower() != 'skip':\n",
    "    try:\n",
    "        import io\n",
    "        \n",
    "        target_ds = client.dataset(upload_choice)\n",
    "        \n",
    "        # Convert to CSV bytes\n",
    "        csv_buffer = io.BytesIO()\n",
    "        df_combined.to_csv(csv_buffer, index=False)\n",
    "        csv_bytes = csv_buffer.getvalue()\n",
    "        \n",
    "        # Upload\n",
    "        file_id = target_ds.files.upload(\"combined_analysis.csv\", csv_bytes)\n",
    "        print(f\"Uploaded! File ID: {file_id}\")\n",
    "        \n",
    "        # Optionally ingest to table\n",
    "        ingest = input(\"Ingest to table? (yes/no): \").strip().lower()\n",
    "        if ingest == 'yes':\n",
    "            target_ds.files.ingest(file_id, opt=\"drop\")\n",
    "            print(\"Ingested to table!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Upload failed: {e}\")\n",
    "else:\n",
    "    print(\"Skipped upload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated multi-dataset analysis techniques:\n",
    "\n",
    "1. **Dataset Discovery**: Find related datasets using STAC API spatial search\n",
    "2. **Data Loading**: Load from ODP or use synthetic demo data\n",
    "3. **Join Strategies**: Exact keys, temporal alignment, spatial binning\n",
    "4. **Cross-Dataset Analysis**: Temperature-species relationships, seasonal patterns\n",
    "5. **Spatial Correlation**: Station-level summaries, biodiversity metrics\n",
    "6. **Export**: Save combined data locally or upload to ODP\n",
    "\n",
    "## Join Strategy Reference\n",
    "\n",
    "| Strategy | When to Use | Pandas Method |\n",
    "|----------|-------------|---------------|\n",
    "| Exact key | Same sampling scheme | `pd.merge(on='key')` |\n",
    "| Temporal | Different time resolution | `pd.merge_asof()` |\n",
    "| Spatial bin | Different locations | Custom binning + merge |\n",
    "| H3 hexagon | Large-scale spatial | `h3.latlng_to_cell()` + merge |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **05_dask_distributed_processing.ipynb**: Scale to large datasets with Dask\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Pandas merge documentation](https://pandas.pydata.org/docs/reference/api/pandas.merge.html)\n",
    "- [ODP Python SDK](https://docs.hubocean.earth/python_sdk/intro/)\n",
    "- [H3 Spatial Indexing](https://h3geo.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
