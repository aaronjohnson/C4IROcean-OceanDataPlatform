{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Processing with Dask\n",
    "\n",
    "This notebook demonstrates how to use Dask for distributed processing of large oceanographic datasets on the Ocean Data Platform.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Set up a Dask client in ODP Workspaces\n",
    "- Process larger-than-memory datasets using Dask DataFrames\n",
    "- Parallelize computations across dataset partitions\n",
    "- Scale aggregations and transformations for large data volumes\n",
    "\n",
    "**Why Dask for Ocean Data?**\n",
    "\n",
    "Research vessels can generate 25-30 TB of data per mission from sensors, ROVs, and sampling systems. Dask enables:\n",
    "- Processing datasets larger than available RAM\n",
    "- Parallel execution across multiple cores/workers\n",
    "- Lazy evaluation - build computation graphs before executing\n",
    "- Familiar pandas-like API\n",
    "\n",
    "**Prerequisites:**\n",
    "- Running in ODP Workspace (Dask sidecar pre-configured)\n",
    "- Completed `01_catalog_discovery.ipynb` to understand dataset access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dask Client Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, progress\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "# Check Dask version\n",
    "print(f\"Dask version: {dask.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize Dask client\n# Use dashboard_address=':0' to auto-select available port (avoids port conflicts)\nclient = Client(dashboard_address=':0')\n\n# Build dashboard URL for cloud environment (jupyter-server-proxy)\nimport os\njupyter_prefix = os.environ.get('JUPYTERHUB_SERVICE_PREFIX', '/')\ndashboard_port = client.scheduler_info().get('services', {}).get('dashboard', 8787)\n\n# Construct full clickable URL for cloud environments\nif 'JUPYTERHUB_SERVICE_PREFIX' in os.environ:\n    proxy_dashboard = f\"https://workspace.hubocean.earth{jupyter_prefix}proxy/{dashboard_port}/status\"\nelse:\n    proxy_dashboard = f\"http://127.0.0.1:{dashboard_port}/status\"\n\nprint(f\"Dashboard: {proxy_dashboard}\")\nprint(\"\\nClick the URL above to open the Dask dashboard.\")\nclient"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Configure data scale for this tutorial\n# Choose based on your VM resources:\n#   small  = 100K rows (~10 MB)  - quick demo, minimal resources\n#   medium = 1M rows   (~100 MB) - recommended for most environments\n#   large  = 10M rows  (~1 GB)   - requires 8GB+ RAM\n\nSCALE = \"medium\"  # Options: \"small\", \"medium\", \"large\"\nUSE_SYNTHETIC = True  # True = generate synthetic data, False = use ODP dataset\n\nscales = {\n    \"small\": 100_000,\n    \"medium\": 1_000_000,\n    \"large\": 10_000_000\n}\nn_rows = scales[SCALE]\n\nprint(f\"Scale: {SCALE} ({n_rows:,} rows)\")\nprint(f\"Data source: {'Synthetic' if USE_SYNTHETIC else 'ODP Dataset'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nfrom datetime import datetime, timedelta\n\nif USE_SYNTHETIC:\n    # Generate synthetic oceanographic time series data\n    print(f\"Generating {n_rows:,} synthetic observations...\")\n    \n    np.random.seed(42)\n    \n    # Simulate 50 monitoring stations\n    stations = [f\"STATION-{i:03d}\" for i in range(50)]\n    \n    # Generate data\n    synthetic_data = {\n        \"timestamp\": pd.date_range(\"2020-01-01\", periods=n_rows, freq=\"T\"),  # Minute-level\n        \"station_id\": np.random.choice(stations, n_rows),\n        \"latitude\": np.random.uniform(58, 72, n_rows),\n        \"longitude\": np.random.uniform(0, 30, n_rows),\n        \"temperature_c\": np.random.normal(8, 4, n_rows),\n        \"salinity_psu\": np.random.normal(35, 1, n_rows),\n        \"depth_m\": np.random.choice([5, 10, 25, 50, 100, 200], n_rows),\n        \"dissolved_oxygen\": np.random.normal(7, 1, n_rows),\n    }\n    \n    source_df = pd.DataFrame(synthetic_data)\n    print(f\"Generated {len(source_df):,} rows, {source_df.memory_usage(deep=True).sum() / 1e6:.1f} MB\")\n    \nelse:\n    # Load from ODP dataset\n    from odp.client import Client as ODPClient\n    \n    odp = ODPClient()\n    DATASET_ID = \"1d801817-742b-4867-82cf-5597673524eb\"  # PGS Biota\n    \n    dataset = odp.dataset(DATASET_ID)\n    stats = dataset.table.stats()\n    print(f\"ODP Dataset: {stats.num_rows:,} rows\" if stats else \"Dataset stats unavailable\")\n    \n    # Load into DataFrame\n    dfs = []\n    for batch in dataset.table.select().batches():\n        dfs.append(batch.to_pandas())\n    source_df = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n    print(f\"Loaded {len(source_df):,} rows from ODP\")\n\nsource_df.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Load Data into Dask\n\nThis notebook supports two data sources:\n\n**Synthetic data (default):** Generates oceanographic time series at configurable scale. Best for learning Dask patterns without API dependencies.\n\n**ODP dataset:** Loads real data from Ocean Data Platform. Use this to apply Dask to actual oceanographic datasets.\n\nThe same Dask patterns work for both - change `USE_SYNTHETIC` above to switch."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert to Dask DataFrame\n# Partition count scales with data size for efficient parallelism\npartitions = max(4, len(source_df) // 100_000)  # ~100K rows per partition\n\nprint(f\"Creating Dask DataFrame with {partitions} partitions...\")\nddf = dd.from_pandas(source_df, npartitions=partitions)\n\nprint(f\"Dask DataFrame: {ddf.npartitions} partitions\")\nprint(f\"Columns: {list(ddf.columns)}\")\nprint(f\"Estimated size: {source_df.memory_usage(deep=True).sum() / 1e6:.1f} MB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data (lazy - only computes first partition)\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lazy Computation with Dask\n",
    "\n",
    "Dask uses lazy evaluation - operations build a task graph that executes only when you call `.compute()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define a computation (lazy - not executed yet)\n# Group by station and count observations\n\nif USE_SYNTHETIC:\n    group_col = 'station_id'\nelse:\n    group_col = 'scientificName' if 'scientificName' in ddf.columns else ddf.columns[0]\n\ngroup_counts = ddf.groupby(group_col).size()\n\n# This just shows the task graph structure\nprint(f\"Task graph created for groupby('{group_col}').size()\")\nprint(f\"Type: {type(group_counts)}\")\nprint(\"(Not yet computed - Dask uses lazy evaluation)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Execute the computation\nresult = group_counts.compute()\n\nprint(f\"\\nObservations by {group_col}:\")\nprint(result.sort_values(ascending=False).head(10))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parallel Aggregations\n",
    "\n",
    "Dask excels at parallel aggregations across partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple aggregations in parallel\n",
    "# Adjust column names based on your dataset schema\n",
    "\n",
    "# Check available numeric columns\n",
    "numeric_cols = ddf.select_dtypes(include=['number']).columns.tolist()\n",
    "print(f\"Numeric columns: {numeric_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Parallel aggregations - statistics by group\nif USE_SYNTHETIC:\n    # Synthetic data: stats by station\n    depth_stats = ddf.groupby('station_id').agg({\n        'temperature_c': ['mean', 'min', 'max'],\n        'depth_m': ['mean', 'count']\n    }).compute()\n    \n    print(\"Temperature and depth statistics by station:\")\n    print(depth_stats.head(10))\n    \nelse:\n    # ODP data: stats by available columns\n    if 'lifeStage' in ddf.columns and 'minimumDepthInMeters' in ddf.columns:\n        depth_stats = ddf.groupby('lifeStage').agg({\n            'minimumDepthInMeters': ['mean', 'min', 'max', 'count']\n        }).compute()\n        print(\"Depth statistics by life stage:\")\n        print(depth_stats)\n    else:\n        print(f\"Available columns: {list(ddf.columns)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parallel Apply for Custom Functions\n",
    "\n",
    "Use `map_partitions` to apply custom functions across partitions in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def process_partition(df):\n    \"\"\"\n    Custom processing function applied to each partition.\n    Example: Extract temporal features and compute derived metrics.\n    \"\"\"\n    result = df.copy()\n    \n    if 'timestamp' in df.columns:\n        result['timestamp'] = pd.to_datetime(result['timestamp'], errors='coerce')\n        result['year'] = result['timestamp'].dt.year\n        result['month'] = result['timestamp'].dt.month\n        result['hour'] = result['timestamp'].dt.hour\n    elif 'eventDate' in df.columns:\n        result['eventDate'] = pd.to_datetime(result['eventDate'], errors='coerce')\n        result['year'] = result['eventDate'].dt.year\n    \n    return result\n\n# Apply function across all partitions in parallel\nprocessed_ddf = ddf.map_partitions(process_partition)\n\n# Aggregate by time period\nif 'year' in processed_ddf.columns:\n    yearly_counts = processed_ddf.groupby('year').size().compute()\n    print(\"Observations by year:\")\n    print(yearly_counts.sort_index())\n    \nif 'month' in processed_ddf.columns:\n    monthly_counts = processed_ddf.groupby('month').size().compute()\n    print(\"\\nObservations by month:\")\n    print(monthly_counts.sort_index())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Memory-Efficient Processing Pattern\n",
    "\n",
    "For very large datasets, process in chunks and aggregate results progressively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_large_dataset_streaming(dataset, chunk_processor, filter_expr=None):\n",
    "    \"\"\"\n",
    "    Process large ODP dataset in streaming fashion with Dask.\n",
    "    \n",
    "    Args:\n",
    "        dataset: ODP dataset\n",
    "        chunk_processor: Function that takes a DataFrame and returns aggregated result\n",
    "        filter_expr: Optional filter\n",
    "    \n",
    "    Returns:\n",
    "        Combined results from all chunks\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    select = dataset.table.select(filter_expr) if filter_expr else dataset.table.select()\n",
    "    \n",
    "    for i, batch in enumerate(select.batches()):\n",
    "        df = batch.to_pandas()\n",
    "        \n",
    "        # Process chunk with Dask (useful for complex operations)\n",
    "        ddf_chunk = dd.from_pandas(df, npartitions=2)\n",
    "        chunk_result = chunk_processor(ddf_chunk)\n",
    "        results.append(chunk_result)\n",
    "        \n",
    "        print(f\"Processed batch {i+1}: {len(df)} rows\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example chunk processor\n",
    "def count_by_species(ddf_chunk):\n",
    "    if 'scientificName' in ddf_chunk.columns:\n",
    "        return ddf_chunk.groupby('scientificName').size().compute()\n",
    "    return pd.Series()\n",
    "\n",
    "# Process dataset\n",
    "print(\"Processing dataset in streaming mode...\")\n",
    "chunk_results = process_large_dataset_streaming(dataset, count_by_species)\n",
    "\n",
    "# Combine results\n",
    "if chunk_results:\n",
    "    combined = pd.concat(chunk_results).groupby(level=0).sum()\n",
    "    print(f\"\\nCombined species counts ({len(combined)} species):\")\n",
    "    print(combined.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Geospatial Processing with Dask\n",
    "\n",
    "Combine ODP's geospatial filtering with Dask's parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regions of interest\n",
    "regions = {\n",
    "    \"north_sea\": \"POLYGON((-5 51, 9 51, 9 62, -5 62, -5 51))\",\n",
    "    \"norwegian_sea\": \"POLYGON((-5 62, 15 62, 15 72, -5 72, -5 62))\",\n",
    "    \"barents_sea\": \"POLYGON((15 68, 40 68, 40 80, 15 80, 15 68))\"\n",
    "}\n",
    "\n",
    "def process_region(dataset, region_name, wkt_polygon, geometry_col='footprintWKT'):\n",
    "    \"\"\"\n",
    "    Process data for a specific geographic region.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use ODP's geospatial filter\n",
    "        filter_expr = f\"{geometry_col} within $area\"\n",
    "        \n",
    "        dfs = []\n",
    "        for batch in dataset.table.select(filter_expr, vars={\"area\": wkt_polygon}).batches():\n",
    "            dfs.append(batch.to_pandas())\n",
    "        \n",
    "        if dfs:\n",
    "            df = pd.concat(dfs, ignore_index=True)\n",
    "            return {\n",
    "                \"region\": region_name,\n",
    "                \"observation_count\": len(df),\n",
    "                \"unique_species\": df['scientificName'].nunique() if 'scientificName' in df.columns else 0\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {region_name}: {e}\")\n",
    "    \n",
    "    return {\"region\": region_name, \"observation_count\": 0, \"unique_species\": 0}\n",
    "\n",
    "# Process regions (could be parallelized with Dask delayed)\n",
    "from dask import delayed\n",
    "\n",
    "# Create delayed tasks for each region\n",
    "delayed_results = [\n",
    "    delayed(process_region)(dataset, name, wkt)\n",
    "    for name, wkt in regions.items()\n",
    "]\n",
    "\n",
    "# Execute in parallel\n",
    "print(\"Processing regions in parallel...\")\n",
    "region_stats = dask.compute(*delayed_results)\n",
    "\n",
    "# Display results\n",
    "region_df = pd.DataFrame(region_stats)\n",
    "print(\"\\nRegion Statistics:\")\n",
    "print(region_df)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Display dashboard links (cloud-friendly via jupyter-server-proxy)\nimport os\njupyter_prefix = os.environ.get('JUPYTERHUB_SERVICE_PREFIX', '/')\ndashboard_port = client.scheduler_info().get('services', {}).get('dashboard', 8787)\n\nif 'JUPYTERHUB_SERVICE_PREFIX' in os.environ:\n    base = f\"https://workspace.hubocean.earth{jupyter_prefix}proxy/{dashboard_port}\"\nelse:\n    base = f\"http://127.0.0.1:{dashboard_port}\"\n\nprint(f\"Dask Dashboard Views:\")\nprint(f\"  Status:    {base}/status\")\nprint(f\"  Tasks:     {base}/tasks\")\nprint(f\"  Workers:   {base}/workers\")\nprint(f\"  Memory:    {base}/memory\")\nprint(f\"  Progress:  {base}/progress\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display dashboard link (cloud-friendly via jupyter-server-proxy)\nimport os\njupyter_prefix = os.environ.get('JUPYTERHUB_SERVICE_PREFIX', '/')\ndashboard_port = client.scheduler_info().get('services', {}).get('dashboard', 8787)\n\nprint(f\"Dask Dashboard: {jupyter_prefix}proxy/{dashboard_port}/status\")\nprint(\"\\nDashboard views to explore:\")\nprint(f\"  Task stream: {jupyter_prefix}proxy/{dashboard_port}/tasks\")\nprint(f\"  Workers:     {jupyter_prefix}proxy/{dashboard_port}/workers\")\nprint(f\"  Memory:      {jupyter_prefix}proxy/{dashboard_port}/memory\")\nprint(f\"  Progress:    {jupyter_prefix}proxy/{dashboard_port}/progress\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cluster status\n",
    "print(\"Cluster Info:\")\n",
    "print(f\"  Workers: {len(client.scheduler_info()['workers'])}\")\n",
    "print(f\"  Total threads: {sum(w['nthreads'] for w in client.scheduler_info()['workers'].values())}\")\n",
    "print(f\"  Total memory: {sum(w['memory_limit'] for w in client.scheduler_info()['workers'].values()) / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Dask client when done\n",
    "client.close()\n",
    "print(\"Dask client closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Dask Client Setup** - Connecting to the ODP Workspace Dask cluster\n",
    "2. **ODP to Dask** - Converting streaming ODP data to Dask DataFrames\n",
    "3. **Lazy Evaluation** - Building computation graphs before execution\n",
    "4. **Parallel Aggregations** - Group-by operations across partitions\n",
    "5. **Custom Processing** - Using `map_partitions` for parallel apply\n",
    "6. **Streaming Pattern** - Memory-efficient processing for large datasets\n",
    "7. **Geospatial + Parallel** - Combining ODP spatial filters with Dask parallelism\n",
    "8. **Monitoring** - Using the Dask dashboard for performance visibility\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **02_geospatial_analysis.ipynb**: H3 hexagonal aggregation and mapping\n",
    "- **03_data_pipeline.ipynb**: File ingest workflows\n",
    "- **04_multi_dataset_join.ipynb**: Cross-dataset analysis\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Dask Documentation](https://docs.dask.org/)\n",
    "- [Dask DataFrame API](https://docs.dask.org/en/stable/dataframe.html)\n",
    "- [ODP Python SDK](https://docs.hubocean.earth/python_sdk/intro/)\n",
    "- [Dask Best Practices](https://docs.dask.org/en/stable/best-practices.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}