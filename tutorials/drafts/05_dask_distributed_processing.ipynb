{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Processing with Dask\n",
    "\n",
    "This notebook demonstrates how to use Dask for distributed processing of large oceanographic datasets on the Ocean Data Platform.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Set up a Dask client in ODP Workspaces\n",
    "- Process larger-than-memory datasets using Dask DataFrames\n",
    "- Parallelize computations across dataset partitions\n",
    "- Scale aggregations and transformations for large data volumes\n",
    "\n",
    "**Why Dask for Ocean Data?**\n",
    "\n",
    "Research vessels can generate 25-30 TB of data per mission from sensors, ROVs, and sampling systems. Dask enables:\n",
    "- Processing datasets larger than available RAM\n",
    "- Parallel execution across multiple cores/workers\n",
    "- Lazy evaluation - build computation graphs before executing\n",
    "- Familiar pandas-like API\n",
    "\n",
    "**Prerequisites:**\n",
    "- Running in ODP Workspace (Dask sidecar pre-configured)\n",
    "- Completed `01_catalog_discovery.ipynb` to understand dataset access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dask Client Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, progress\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "# Check Dask version\n",
    "print(f\"Dask version: {dask.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Dask client\n",
    "# In ODP Workspaces, this connects to the pre-configured Dask scheduler\n",
    "client = Client()\n",
    "\n",
    "# Display cluster info\n",
    "print(f\"Dashboard: {client.dashboard_link}\")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connect to ODP Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odp.client import Client as ODPClient\n",
    "\n",
    "# Initialize ODP client\n",
    "odp = ODPClient()\n",
    "\n",
    "# Use a dataset with substantial data volume\n",
    "# GLODAP - globally calibrated ocean carbon data (1M+ points)\n",
    "DATASET_ID = \"1d801817-742b-4867-82cf-5597673524eb\"  # PGS Biota - adjust as needed\n",
    "\n",
    "dataset = odp.dataset(DATASET_ID)\n",
    "\n",
    "# Check dataset size\n",
    "stats = dataset.table.stats()\n",
    "if stats:\n",
    "    print(f\"Dataset rows: {stats.num_rows:,}\")\n",
    "    print(f\"Dataset size: {stats.size / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Streaming ODP Data into Dask\n",
    "\n",
    "The ODP SDK supports streaming via `.batches()` which returns PyArrow RecordBatches. We can convert these to a Dask DataFrame for distributed processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odp_to_dask(dataset, filter_expr=None, cols=None, partitions=4):\n",
    "    \"\"\"\n",
    "    Stream ODP dataset into a Dask DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        dataset: ODP dataset object\n",
    "        filter_expr: Optional filter expression\n",
    "        cols: Optional list of columns to select\n",
    "        partitions: Number of partitions to create\n",
    "    \n",
    "    Returns:\n",
    "        Dask DataFrame\n",
    "    \"\"\"\n",
    "    # Collect batches into pandas DataFrames\n",
    "    dfs = []\n",
    "    \n",
    "    select_args = {}\n",
    "    if filter_expr:\n",
    "        select_args['filter'] = filter_expr\n",
    "    if cols:\n",
    "        select_args['cols'] = cols\n",
    "    \n",
    "    for batch in dataset.table.select(**select_args).batches():\n",
    "        dfs.append(batch.to_pandas())\n",
    "    \n",
    "    if not dfs:\n",
    "        return None\n",
    "    \n",
    "    # Concatenate and convert to Dask\n",
    "    full_df = pd.concat(dfs, ignore_index=True)\n",
    "    dask_df = dd.from_pandas(full_df, npartitions=partitions)\n",
    "    \n",
    "    return dask_df\n",
    "\n",
    "# Load dataset into Dask DataFrame\n",
    "print(\"Loading dataset into Dask...\")\n",
    "ddf = odp_to_dask(dataset, partitions=4)\n",
    "\n",
    "if ddf is not None:\n",
    "    print(f\"Dask DataFrame created with {ddf.npartitions} partitions\")\n",
    "    print(f\"Columns: {list(ddf.columns)}\")\n",
    "else:\n",
    "    print(\"No data loaded - check dataset ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data (lazy - only computes first partition)\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lazy Computation with Dask\n",
    "\n",
    "Dask uses lazy evaluation - operations build a task graph that executes only when you call `.compute()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a computation (lazy - not executed yet)\n",
    "# Example: Count observations by scientific name\n",
    "species_counts = ddf.groupby('scientificName').size()\n",
    "\n",
    "# This just shows the task graph structure\n",
    "print(\"Task graph created (not yet computed)\")\n",
    "print(f\"Type: {type(species_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the computation\n",
    "result = species_counts.compute()\n",
    "\n",
    "print(f\"\\nSpecies observation counts:\")\n",
    "print(result.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parallel Aggregations\n",
    "\n",
    "Dask excels at parallel aggregations across partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple aggregations in parallel\n",
    "# Adjust column names based on your dataset schema\n",
    "\n",
    "# Check available numeric columns\n",
    "numeric_cols = ddf.select_dtypes(include=['number']).columns.tolist()\n",
    "print(f\"Numeric columns: {numeric_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Statistics by life stage (if column exists)\n",
    "if 'lifeStage' in ddf.columns and 'minimumDepthInMeters' in ddf.columns:\n",
    "    depth_stats = ddf.groupby('lifeStage').agg({\n",
    "        'minimumDepthInMeters': ['mean', 'min', 'max', 'count']\n",
    "    }).compute()\n",
    "    \n",
    "    print(\"Depth statistics by life stage:\")\n",
    "    print(depth_stats)\n",
    "else:\n",
    "    print(\"Columns 'lifeStage' or 'minimumDepthInMeters' not found.\")\n",
    "    print(f\"Available columns: {list(ddf.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parallel Apply for Custom Functions\n",
    "\n",
    "Use `map_partitions` to apply custom functions across partitions in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_partition(df):\n",
    "    \"\"\"\n",
    "    Custom processing function applied to each partition.\n",
    "    Example: Extract year from eventDate and count observations.\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    if 'eventDate' in df.columns:\n",
    "        # Parse dates and extract year\n",
    "        result['eventDate'] = pd.to_datetime(result['eventDate'], errors='coerce')\n",
    "        result['year'] = result['eventDate'].dt.year\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply function across all partitions in parallel\n",
    "processed_ddf = ddf.map_partitions(process_partition)\n",
    "\n",
    "# Check result\n",
    "if 'year' in processed_ddf.columns:\n",
    "    yearly_counts = processed_ddf.groupby('year').size().compute()\n",
    "    print(\"Observations by year:\")\n",
    "    print(yearly_counts.sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Memory-Efficient Processing Pattern\n",
    "\n",
    "For very large datasets, process in chunks and aggregate results progressively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_large_dataset_streaming(dataset, chunk_processor, filter_expr=None):\n",
    "    \"\"\"\n",
    "    Process large ODP dataset in streaming fashion with Dask.\n",
    "    \n",
    "    Args:\n",
    "        dataset: ODP dataset\n",
    "        chunk_processor: Function that takes a DataFrame and returns aggregated result\n",
    "        filter_expr: Optional filter\n",
    "    \n",
    "    Returns:\n",
    "        Combined results from all chunks\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    select = dataset.table.select(filter_expr) if filter_expr else dataset.table.select()\n",
    "    \n",
    "    for i, batch in enumerate(select.batches()):\n",
    "        df = batch.to_pandas()\n",
    "        \n",
    "        # Process chunk with Dask (useful for complex operations)\n",
    "        ddf_chunk = dd.from_pandas(df, npartitions=2)\n",
    "        chunk_result = chunk_processor(ddf_chunk)\n",
    "        results.append(chunk_result)\n",
    "        \n",
    "        print(f\"Processed batch {i+1}: {len(df)} rows\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example chunk processor\n",
    "def count_by_species(ddf_chunk):\n",
    "    if 'scientificName' in ddf_chunk.columns:\n",
    "        return ddf_chunk.groupby('scientificName').size().compute()\n",
    "    return pd.Series()\n",
    "\n",
    "# Process dataset\n",
    "print(\"Processing dataset in streaming mode...\")\n",
    "chunk_results = process_large_dataset_streaming(dataset, count_by_species)\n",
    "\n",
    "# Combine results\n",
    "if chunk_results:\n",
    "    combined = pd.concat(chunk_results).groupby(level=0).sum()\n",
    "    print(f\"\\nCombined species counts ({len(combined)} species):\")\n",
    "    print(combined.sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Geospatial Processing with Dask\n",
    "\n",
    "Combine ODP's geospatial filtering with Dask's parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regions of interest\n",
    "regions = {\n",
    "    \"north_sea\": \"POLYGON((-5 51, 9 51, 9 62, -5 62, -5 51))\",\n",
    "    \"norwegian_sea\": \"POLYGON((-5 62, 15 62, 15 72, -5 72, -5 62))\",\n",
    "    \"barents_sea\": \"POLYGON((15 68, 40 68, 40 80, 15 80, 15 68))\"\n",
    "}\n",
    "\n",
    "def process_region(dataset, region_name, wkt_polygon, geometry_col='footprintWKT'):\n",
    "    \"\"\"\n",
    "    Process data for a specific geographic region.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use ODP's geospatial filter\n",
    "        filter_expr = f\"{geometry_col} within $area\"\n",
    "        \n",
    "        dfs = []\n",
    "        for batch in dataset.table.select(filter_expr, vars={\"area\": wkt_polygon}).batches():\n",
    "            dfs.append(batch.to_pandas())\n",
    "        \n",
    "        if dfs:\n",
    "            df = pd.concat(dfs, ignore_index=True)\n",
    "            return {\n",
    "                \"region\": region_name,\n",
    "                \"observation_count\": len(df),\n",
    "                \"unique_species\": df['scientificName'].nunique() if 'scientificName' in df.columns else 0\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {region_name}: {e}\")\n",
    "    \n",
    "    return {\"region\": region_name, \"observation_count\": 0, \"unique_species\": 0}\n",
    "\n",
    "# Process regions (could be parallelized with Dask delayed)\n",
    "from dask import delayed\n",
    "\n",
    "# Create delayed tasks for each region\n",
    "delayed_results = [\n",
    "    delayed(process_region)(dataset, name, wkt)\n",
    "    for name, wkt in regions.items()\n",
    "]\n",
    "\n",
    "# Execute in parallel\n",
    "print(\"Processing regions in parallel...\")\n",
    "region_stats = dask.compute(*delayed_results)\n",
    "\n",
    "# Display results\n",
    "region_df = pd.DataFrame(region_stats)\n",
    "print(\"\\nRegion Statistics:\")\n",
    "print(region_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Monitoring and Performance\n",
    "\n",
    "Use the Dask dashboard to monitor task execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dashboard link\n",
    "print(f\"Dask Dashboard: {client.dashboard_link}\")\n",
    "print(\"\\nOpen this URL to monitor:\")\n",
    "print(\"- Task stream: real-time task execution\")\n",
    "print(\"- Progress: overall computation progress\")\n",
    "print(\"- Workers: resource utilization per worker\")\n",
    "print(\"- Memory: memory usage across cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cluster status\n",
    "print(\"Cluster Info:\")\n",
    "print(f\"  Workers: {len(client.scheduler_info()['workers'])}\")\n",
    "print(f\"  Total threads: {sum(w['nthreads'] for w in client.scheduler_info()['workers'].values())}\")\n",
    "print(f\"  Total memory: {sum(w['memory_limit'] for w in client.scheduler_info()['workers'].values()) / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Dask client when done\n",
    "client.close()\n",
    "print(\"Dask client closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Dask Client Setup** - Connecting to the ODP Workspace Dask cluster\n",
    "2. **ODP to Dask** - Converting streaming ODP data to Dask DataFrames\n",
    "3. **Lazy Evaluation** - Building computation graphs before execution\n",
    "4. **Parallel Aggregations** - Group-by operations across partitions\n",
    "5. **Custom Processing** - Using `map_partitions` for parallel apply\n",
    "6. **Streaming Pattern** - Memory-efficient processing for large datasets\n",
    "7. **Geospatial + Parallel** - Combining ODP spatial filters with Dask parallelism\n",
    "8. **Monitoring** - Using the Dask dashboard for performance visibility\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **02_geospatial_analysis.ipynb**: H3 hexagonal aggregation and mapping\n",
    "- **03_data_pipeline.ipynb**: File ingest workflows\n",
    "- **04_multi_dataset_join.ipynb**: Cross-dataset analysis\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Dask Documentation](https://docs.dask.org/)\n",
    "- [Dask DataFrame API](https://docs.dask.org/en/stable/dataframe.html)\n",
    "- [ODP Python SDK](https://docs.hubocean.earth/python_sdk/intro/)\n",
    "- [Dask Best Practices](https://docs.dask.org/en/stable/best-practices.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
